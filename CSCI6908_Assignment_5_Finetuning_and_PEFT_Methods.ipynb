{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Siddikpatel/assignment-5/blob/main/CSCI6908_Assignment_5_Finetuning_and_PEFT_Methods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fMlKPz2xzQ8"
      },
      "source": [
        "# Assignment 5: Finetuning and PEFT Methods.\n",
        "<b style=\"color:red;bold;\">Due date: April 2nd, 2025</b>\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In the previous assignment, you explored the inner workings of a Transformer model by pretraining it from scratch. While this process can be valuable in certain scenarios, it may not be practical in many others. The primary reason is that for a model to acquire a substantial amount of knowledge, it requires vast amounts of data coupled with a model size of billions of parameters. This approach can be prohibitively expensive when using consumer-grade hardware.  \n",
        "\n",
        "An alternative paradigm is the pretrain-finetune approach. In this method, a model that has already been pretrained (typically by entities with access to large computing infrastructure, such as tech companies) is then adapted (or finetuned) for a specific task.  \n",
        "\n",
        "While the pretrain-finetune approach offers significant advantages over training from scratch, it still presents challenges when working with large language models. Finetuning the entire model requires substantial computational resources. It also acan lead to catastrophic forgetting of previously learned knowledge. To address these limitations, Parameter-Efficient Fine-Tuning (PEFT) methods have been developed. PEFT techniques allow for the adaptation of pre-trained models to specific tasks while updating only a small subset of the model's parameters. This approach drastically reduces the computational and memory requirements for fine-tuning, making it feasible to adapt large language models on consumer-grade hardware. Moreover, PEFT methods help preserve the model's general knowledge while effectively learning task-specific information, striking a balance between efficiency and performance.  \n",
        "\n",
        "In this assignment, you will run a battery of experiments to compare the effect of different finetuning strategies on a model's performance given a downstream task.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqXp0B7cZmHc"
      },
      "source": [
        "## Part 0: Preliminary\n",
        "\n",
        "### Libraries\n",
        "\n",
        "The assignment mainly relies on two libraries developped by Huggingface:\n",
        "- `transformers`: A comprehensive library that provides state-of-the-art pre-trained models for Natural Language Processing (NLP) and other tasks, offering easy-to-use interfaces for implementing, and fine-tuning a wide range of transformer-based architectures.  \n",
        "- `datasets`: A library for accessing NLP datasets. (Note that you can skip using this library, however, it is very handy when it comes to loading and preprocessing public datasets).\n",
        "- `peft`: A specialized library designed to implement various parameter-efficient fine-tuning methods.\n",
        "\n",
        "In case you are unfamiliar with these libraries, you can consult these resources:\n",
        "- [Stanford's CS224N transformers tutorial](https://colab.research.google.com/drive/1pxc-ehTtnVM72-NViET_D2ZqOlpOi2LH?usp=sharing)\n",
        "- [The `peft`'s library documentation page](https://huggingface.co/docs/peft/en/index)\n",
        "- [The `datasets` library documentation page](https://huggingface.co/docs/datasets/quickstart#nlp)\n",
        "\n",
        "### (!New!) Required Reading Material\n",
        "Compared to other assignments, you will need to go over the following reading materials:\n",
        "- Video: [EMNLP 2022 Tutorial - \"Modular and Parameter-Efficient Fine-Tuning for NLP Models\"](https://youtu.be/KoOlcX3XLd4?si=jKh2FCwETnrydJND)\n",
        "- Blog: [PEFT - Parameter Efficient Fine-Tuning: A Survey](https://aiwithmike.substack.com/p/peft-parameter-efficient-fine-tuning?utm_campaign=post&utm_medium=web)\n",
        "\n",
        "These materials provide a comprehensive overview and recent advances in the domain of PEFT methods. **The quiz will include questions related to some of the concepts addressed there**.\n",
        "\n",
        "### Model\n",
        "\n",
        "In this assignment you will use the RoBERTa base model. It roughly has 125M parameters. It bears similar architecture to BERT. The difference lays in the pretraining tasks. You can find the model on Huggingface's [hub](https://huggingface.co/FacebookAI/roberta-base). If you want, you can choose a bigger model. However, you cannot choose a model whose parameter size is less than RoBERTa's.\n",
        "\n",
        "### Task\n",
        "\n",
        "The General Language Understanding Evaluation (GLUE) benchmark is a collection of diverse natural language understanding tasks designed to evaluate and compare the performance of machine learning models across a range of language processing challenges. In this assignment, you will finetune and evaluate the aforemention model on the Corpus of Linguistic Acceptability (CoLA) task. CoLA is a collection of English sentences labeled as either grammatically acceptable or unacceptable, used to evaluate a model's ability to distinguish between well-formed and ill-formed sentences. It is basically a binary classification task. You can find the dataset on Huggingface's [hub](https://huggingface.co/datasets/nyu-mll/glue/viewer/cola).\n",
        "\n",
        "\n",
        "### Recommendations\n",
        "This assignment involves finetuning a model multiple times under different settings. This might take a while, so make sure to start working on the assignment early. Second, you may be working on the free tier of Colab, which means you will have limited access to the GPU it offers. Hence, make sure to save the output of each cell regularly so that you do not have to rerun it again (whenever this is applicable). Third, in the majority of questions, you will be asked to report results after running multiple experiments. This might not be possible to do within one session since you may loose access to Colab's GPUs for a certain period of time (another reason why you should start early!). To overcome this, make sure that you save the results of each experiment locally, so that when you start a new session, you do not have to rerun everything from the beginning.\n",
        "\n",
        "## Submission Guidelines\n",
        "- Add the TA as _Maintainer_ to your fork and submit your fork's URL on Brightspace. Push the modifications you have made on this notebook to your fork.  \n",
        "- **Please note that a missing Brightspace submission will be treated as a non-submissiom, even if you have added the TA to your fork.**  \n",
        "- **If you have stored raw results locally (e.g. in CSV or JSON files), you need to push those as well.**  \n",
        "- **It is important that the cells of the Jupyter notebook reflect what is required in each part. For example, if a question asks you to finetune a model with varrying number of frozen layers (e.g. 2, 4 and 6), you should create three cells where each one corresponds to each setting. Becareful, to not override the same cell.**\n",
        "\n",
        "## Submission of SDAs\n",
        "For this assignment, SDAs should be submitted through Brightspace. You will find the submission box under _Assignments_ > _SDA_. **<span style=\"color:red\">Please note that you are only allowed to submit 2 SDAs for this course. Hence, if you already reached that limit, you can no longer submit one. In addition, no SDA will be considered if sent by email. All SDAs should be submitted through Brightspace at least 24 hours prior to the deadline. The PDF file should be following this format: `<BANNER-ID>_<FIRST-NAME>_<LAST_NAME>_<ASSIGNMENT-NUMBER>.pdf`</span>**  \n",
        "\n",
        "If you have any questions, please do not hesitate to post them on the courseâ€™s Teams channel. If you have not joined already, you can do so by using this code: `ot45hmp`.\n",
        "\n",
        "Good luck!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwXfGpMGCwJ5"
      },
      "source": [
        "## Part 1: Full Finetuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hECEzxaUHa1D"
      },
      "source": [
        "In this part, you are tasked with fully fine-tuning the RoBERTa model, or the model that you have chosen, on the CoLA dataset. You may utilize the built-in classes for model definition and training provided by the `transformers` library. It is worth noting that many of these classes, particularly those used to load model weights from the hub, are essentially PyTorch classes at their core. This means they can serve as building blocks for other PyTorch models, offering you flexibility in your implementation. The decision of whether to use the `transformers` classes extensively or to employ only a subset of them to tackle the CoLA task is left to your.  \n",
        "\n",
        "Given the nature of the task, choose one adequate performance metric. In addition, report the time taken to finetune the model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7MYh7x6CBRN",
        "outputId": "9ff64d52-5b50-4e13-fe90-49fd7fda15b4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.14)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.29.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "import numpy as np\n",
        "import time\n",
        "from datetime import timedelta\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "8hemYkk0Bs2d"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    mcc = matthews_corrcoef(labels, predictions)\n",
        "    return {\"matthews_correlation\": mcc}"
      ],
      "metadata": {
        "id": "tRXKPRGgCish"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=64,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=100,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"matthews_correlation\",\n",
        "    greater_is_better=True,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrRZn1ulDDO1",
        "outputId": "4c04e317-853d-45a4-ba36-ac1d4f4da448"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"sentence\"], padding=\"max_length\", truncation=True, max_length=128)"
      ],
      "metadata": {
        "id": "SnmVOSucEJv5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"roberta-base\"\n",
        "tokenizer = RobertaTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "iOi_DOfHF31A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b84d0a9d-0c27-4f6d-8d79-8772feb8ff1a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"glue\", \"cola\")\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "train_dataset = tokenized_datasets[\"train\"]\n",
        "eval_dataset = tokenized_datasets[\"validation\"]"
      ],
      "metadata": {
        "id": "7IthtVmnEOBI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"roberta-base\"\n",
        "tokenizer = RobertaTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "8HZ8rvVXIT1R"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SJ7khOOsGPJH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "outputId": "c4f49e14-1987-41ed-dee6-ecb618c19e05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpatel-siddik369\u001b[0m (\u001b[33mpatel-siddik369-no\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250326_153651-hhjlrkg5</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/patel-siddik369-no/huggingface/runs/hhjlrkg5' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/patel-siddik369-no/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/patel-siddik369-no/huggingface' target=\"_blank\">https://wandb.ai/patel-siddik369-no/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/patel-siddik369-no/huggingface/runs/hhjlrkg5' target=\"_blank\">https://wandb.ai/patel-siddik369-no/huggingface/runs/hhjlrkg5</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='268' max='268' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [268/268 2:59:06, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Matthews Correlation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.461900</td>\n",
              "      <td>0.429097</td>\n",
              "      <td>0.574557</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [17/17 05:55]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "====================\n",
            "PERFORMANCE REPORT\n",
            "====================\n",
            "Model: roberta-base\n",
            "Training time: 3:00:04.989456\n",
            "Matthews Correlation Coefficient (MCC): 0.5746\n",
            "====================\n"
          ]
        }
      ],
      "source": [
        "model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "train_results = trainer.train()\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "training_time = end_time - start_time\n",
        "training_time_formatted = str(timedelta(seconds=training_time))\n",
        "\n",
        "eval_results = trainer.evaluate()\n",
        "\n",
        "mcc_score = eval_results[\"eval_matthews_correlation\"]\n",
        "\n",
        "print(\"\\n\" + \"=\"*20)\n",
        "print(\"PERFORMANCE REPORT\")\n",
        "print(\"=\"*20)\n",
        "print(f\"Model: {model_name}\")\n",
        "print(f\"Training time: {training_time_formatted}\")\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc_score:.4f}\")\n",
        "print(\"=\"*20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6CsZCXjC3aV"
      },
      "source": [
        "## Part 2: Layer Freezing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8TN4UvCLrch"
      },
      "source": [
        "In this part of the assignment, you will fine-tune RoBERTa using layer freezing and analyze its impact on performance.  \n",
        "\n",
        "Layer freezing involves keeping certain layers of a pre-trained model fixed during fine-tuning, updating only the parameters of the unfrozen layers.\n",
        "\n",
        "1. Your task is to experiment with freezing the first $k$ layers of the RoBERTa model, where $k$ takes the values 2, 4, 6, 8, and 10. For example, when $k$ = 6, freeze the first 6 layers of the model, leaving the remaining layers trainable. To freeze a layer in PyTorch, set the `requires_grad` attribute of its parameters to `False`. This prevents gradient computation for these parameters during backpropagation. Implement this freezing strategy for each value of $k$, fine-tune the model on the CoLA dataset, and evaluate its performance on the test set. You will also need to calculate the time taken to finetune the model under each value of $k$.\n",
        "\n",
        "2. After completing all experiments, you will need to plot two lineplots. The first will be the performance as a function of the number of frozen layers ($k$). The second plot will be the running time as function of $k$. Describe both of plots and state whether a possible relationship exists, and if so, does it make sense and why."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "TptMBoavMMfQ"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "results_file = \"results.json\"\n",
        "\n",
        "if os.path.exists(results_file):\n",
        "    with open(results_file, \"r\") as f:\n",
        "        saved_results = json.load(f)\n",
        "        saved_k_values = saved_results.get(\"k_values\", [])\n",
        "        saved_performance = saved_results.get(\"performance_results\", [])\n",
        "        saved_training_times = saved_results.get(\"training_times\", [])\n",
        "else:\n",
        "    saved_k_values = []\n",
        "    saved_performance = []\n",
        "    saved_training_times = []"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to freeze the first k layers of the model\n",
        "def freeze_layers(model, k):\n",
        "    for param in model.roberta.embeddings.parameters():\n",
        "        param.requires_grad = False\n",
        "    for i in range(k):\n",
        "        for param in model.roberta.encoder.layer[i].parameters():\n",
        "            param.requires_grad = False"
      ],
      "metadata": {
        "id": "KUasD7keNJoC"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k_values = [2, 4, 6, 8, 10]\n",
        "\n",
        "for k in k_values:\n",
        "    if k in saved_k_values:\n",
        "        print(f\"k={k}, already computed.\")\n",
        "        continue\n",
        "\n",
        "    # Reload the model to reset weights\n",
        "    model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "\n",
        "    # Freeze the first k layers\n",
        "    freeze_layers(model, k)\n",
        "\n",
        "    # Create a new trainer with the updated model\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    start_time = time.time()\n",
        "    trainer.train()\n",
        "    end_time = time.time()\n",
        "\n",
        "    training_time = str(timedelta(end_time - start_time))\n",
        "    eval_results = trainer.evaluate()\n",
        "    mcc_score = eval_results[\"eval_matthews_correlation\"]\n",
        "\n",
        "    # Append new results\n",
        "    saved_k_values.append(k)\n",
        "    saved_performance.append(mcc_score)\n",
        "    saved_training_times.append(training_time)\n",
        "\n",
        "    # Save updated results to file\n",
        "    with open(results_file, \"w\") as f:\n",
        "        json.dump(\n",
        "            {\n",
        "                \"k_values\": saved_k_values,\n",
        "                \"performance_results\": saved_performance,\n",
        "                \"training_times\": saved_training_times,\n",
        "            },\n",
        "            f,\n",
        "            indent=4,\n",
        "        )\n",
        "\n",
        "    print(f\"Completed k={k}, MCC={mcc_score}, Time={training_time}s\")\n",
        "\n",
        "# Plot results after all experiments\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(saved_k_values, saved_performance, marker=\"o\")\n",
        "plt.xlabel(\"Frozen Layers (k)\")\n",
        "plt.ylabel(\"Matthews Correlation Coefficient (MCC)\")\n",
        "plt.title(\"Performance vs. Number of Frozen Layers\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(saved_k_values, saved_training_times, marker=\"o\", color=\"r\")\n",
        "plt.xlabel(\"Frozen Layers (k)\")\n",
        "plt.ylabel(\"Training Time (seconds)\")\n",
        "plt.title(\"Training Time vs. Number of Frozen Layers\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "wl1b8GzFNMVO",
        "outputId": "b6c05a67-6026-4ac0-c281-688b17c0aacc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "k=2, already computed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='268' max='268' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [268/268 2:14:12, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Matthews Correlation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.441400</td>\n",
              "      <td>0.434882</td>\n",
              "      <td>0.548133</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [17/17 05:55]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed k=4, MCC=0.5481326292844919, Time=8088.1782965660095s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='268' max='268' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [268/268 1:56:44, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Matthews Correlation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.448300</td>\n",
              "      <td>0.450634</td>\n",
              "      <td>0.521376</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [17/17 05:57]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed k=6, MCC=0.5213763355102656, Time=7032.553395748138s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='210' max='268' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [210/268 1:11:45 < 20:00, 0.05 it/s, Epoch 0.78/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4Pot1OsHWBr"
      },
      "source": [
        "## Part 3: Prefix Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9che01oWRCgg"
      },
      "source": [
        "Now you will explore prefix tuning, another PEFT technique for large language models. Prefix tuning involves prepending a trainable prefix to the input of each transformer layer while keeping the pre-trained model parameters frozen. This approach allows for task-specific adaptation with a significantly smaller number of trainable parameters compared to full fine-tuning. Your task is to implement prefix tuning for the RoBERTa model on the CoLA dataset.\n",
        "\n",
        "1. Use the `peft` library to add trainable prefixes to each layer of the model. Specifically, experiment with different prefix lengths $L$ = 10, 30, 50, 100.\n",
        "2. Draw two line plots showing the relationship between prefix length and model performance on the test set, and between the prefix length and finetuning time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cX_qYTcGFEO"
      },
      "source": [
        "## Part 4: LoRA: Low-Rank Adaptation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fx5xy3-11p-"
      },
      "source": [
        "The final part is to experiment with finetuning RoBERTa using LoRA. Similar to the previous parts of the assignment, you will experiment with different hyperparameters and investigate their impact on the model's downstream performance and the required time to complete the finetuning.  \n",
        "Specifically, you will need to try **three** distinct combinations of rank $(r)$ and $\\alpha$ values. It is up to you to choose which values these hyperparameters can take.  \n",
        "\n",
        "Also, similar to the previous parts, you will need to provide a visualization of the impact of hyperparameter choice on the performance and speed. Now givem that LoRA has more hyperparameters compared to the other methods, the choice of plots is up to you. For instance, you could use a heatmap where the rank is on the x-axis, alpha is on the y-axis, and cell values would represent the performance metric, and in the other plot would have the finetuning time as the cell values. Alternatively, you can also use 3D line plots. What matters is that the relationship between hyperparameters and these metrics can be visualized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMd3ICMcxRVl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoTilftw11p_"
      },
      "source": [
        "## Part 5: Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPYsGNX8Sdwr"
      },
      "source": [
        "Based on your experiments and the results you have obtained when performing full fine-tuning, layer freezing, prefix tuning, and LoRA, what conclusions can you draw about parameter-efficient fine-tuning techniques when applied on RoBERTa?\n",
        "Specifically, do the PEFT methods offer an advantage over full finetuning or not? and why is that the case?  \n",
        "\n",
        "**N.S:** Note that there is not one correct answer. What is important however, is to justify your claims with data (e.g., experiment results)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGakPKDBSjez"
      },
      "source": [
        "## Rubric (/10):\n",
        "### Part 1: Full finetuning: (/2)\n",
        "- Correct implementation of full finetuning: /1\n",
        "- Correct choice of the performance metric: /0.5\n",
        "- Report of the performance metric and the finetuning time: /0.5\n",
        "### Part 2: Layer freezing: (/2)\n",
        "- Correct implementation of layer freezing across all possible $k$ values: /1\n",
        "- Correct rendering of plots and their discussion: /1\n",
        "### Part 3: Prefix Tuning: (/2)\n",
        "- Correct implementation of finetuning using prefix tuning: /1\n",
        "- Line plot of prefix length vs. model performance and the line plot of prefix length vs. finetuning time: /1\n",
        "### Part 4: LoRRA (/2):\n",
        "- Correct implementation of finetuning using LoRA: /1\n",
        "- Plots that showcase the relationship between hyperparameters and metrics: /1\n",
        "### Part 5: Discussion: (/2)\n",
        "- Properly articulated claims backed with empirical data: /2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQGOu3s311qA"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68P2o9sq11qA"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}